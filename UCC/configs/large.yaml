
# use cpu for easy debug
device: cuda
seed: 42
n_epochs: 1
# for LM
n_epochs_early_stage: 4
#n_epochs_early_stage: 0
#batch_size: 64
# for LM
batch_size: 1280
#limit_example_length: 100000
limit_example_length: 
# lm context size is equal max_seq_length
max_seq_length: 15
# must be even, and >= 2
max_context_size: 6
shuffle_data: True
# in word emb mode, english letter will removed in max_vocab_size by utils.vocab_zh_trim_rule
max_vocab_size: 42000
pretrain_emb: True

emb_freeze: True
enc_dropout: 0.1
dec_dropout: 0.1
num_layers: 6
n_head: 8
d_model: 512
d_ff: 2048
attn_alpha: 1
adapter_d_ff: 2048

lr: 1.5e-4
weight_decay: 0.05
clip_grad: 1
use_scheduler: False
warmup_steps: 800
gradient_accumulation: 1
adapter_finetune: False

model_path: models/
# lm clip_grad
#pretrained_path:
pretrained_path: models/model_lm_epoch4_all/model.pt
# 31476781 based on 2m
#pretrained_path: models/model_lm_epoch5_2m_all/model.pt
#pretrained_path: models/model_lm_epoch9_2m/model.pt
# data_path: tmp/
data_path: datas/
cache_path: caches/
corpus_fname:
vec_fname: models/vec-char.txt
vocab_fname: models/vocab-char.txt


