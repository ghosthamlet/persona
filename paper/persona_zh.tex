\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{natbib}
\usepackage{hyperref}       % hyperlinks
% https://mirrors.tuna.tsinghua.edu.cn/CTAN/macros/latex/contrib/hyperref/doc/backref.pdf
\usepackage[hyperpageref]{backref}

\usepackage{amsmath}
\usepackage{IEEEtrantools}

% \usepackage[sort&compress,square,comma,numbers]{natbib}

\usepackage{CJKutf8}
\AtBeginDvi{\input{zhwinfonts}}

% \DeclareRobustCommand{\citeext}[1]{\citeauthor{#1}~\cite{#1}}
\DeclareRobustCommand{\citeext}[1]{\cite[#1]{#1}}

\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
\setcounter{secnumdepth}{0}  
\linespread{1.2}

\begin{document}
\begin{CJK*}{UTF8}{gbsn}

\title{开放领域个性化对话生成专用模型的改进}
\author{胡俊波，韩强，刘华伟 \\
杭州高登影视公司\\
% \author{Jing Bo Hu, Qiang Han~\& Hua Wei Liu \\
% HuggingFace  Inc.\\
{\tt \{hjb,gvvvv,lhw\}@163.com} \\}

\maketitle
\begin{abstract}
本论文主要分析Transformer架构近几年来的一些速度和性能上的改进方法，其在专用模型训练上的运用，这里研究的专用模型是指开放领域个性化对话生成模型，针对的数据集是多轮短对话, 单个输入系列总长度不超过105个字，因此Transformer架构在长文本处理上的架构、注意力机制等方面的诸多改进不在本论文讨论之列。
\end{abstract}

\section[Background]{背景} 
NLP(自然语言处理)领域的革命从注意力机制\citeext{Bahdanau2015}的奠基，~Transformer架构\citeext{Vaswani2017}点燃了导火索，到BERT模型\citeext{Devlin2019}正式拉开革命帷幕，以简单的纯注意力机制架构取代了曾经是NLP主流的相对复杂的RNN架构系列模型，并在模型性能、扩展性以及速度上都取得了极大的提升，４年间全面改变了NLP领域，造就了NLP领域的ImageNet时刻\citeext{ruder2018nlpimagenet}。

开放领域对话生成是NLP的一项重要且十分复杂的任务，Transformer架构在这项任务上同样取得了质的飞跃，如GPT-2、GPT-3模型和Google Meena\citeext{Radford2019, Brown2020, Adiwardana2020}。后文如无指定说明，涉及模型皆为Transformer架构模型。

GPT-2、GPT-3模型都是通用模型，适用于几乎所有NLP任务，并非专用于开放领域对话生成，这类模型通常需以巨大的模型尺寸、海量预训练数据和漫长的预训练时间为代价，才能在性能上与专用中小模型相匹配（尽管有DistilGPT2\footnote{\url{https://github.com/huggingface/transformers/tree/master/examples/distillation}}，DistilBERT\citeext{Sanh2019}对通用模型的优化，但其性能也相应下降）。
而且通用模型很难生成个性一致的对话\footnote{即聊天机器人带有明确固定个性特征如姓名性别地址等等的个性化对话}，即使如微调GPT-2模型\citeext{Radford2019}的方法TransferTransfo\citeext{Wolf2019}\footnote{在输入的对话上下文中增加了个性化数据}，虽然其在个性化对话英文数据集PERSONA-CHAT\citeext{Zhang2018}上表现不错，但在个性化对话中文数据集PersonalDialog\citeext{Zheng2019a}上测试时仍然不如专用模型\citeext{Zheng2019}。GPT-3只有Beta API，无需微调，用提示编程方式前置个性字符串即可，但我们没有取得测试权限，无法对比性能。

本论文即对开放领域个性化对话生成专用模型\citeext{Zheng2019}引入Transformer架构近几年的速度和性能上的一些改进，并分析在此专用模型上有效和无效的各种尝试，实验的源代码已经开源\footnote{\url{https://github.com/ghosthamlet/persona}}。后文简称开放领域个性化对话生成为个性化对话生成。

\section[Related Works]{相关研究} 
个性化对话生成的研究相对于无个性对话生成和闭合领域任务型对话生成来说比较少，因此相关的对话数据集也较少。英文的主要是PERSONA-CHAT\citeext{Zhang2018}，中文的有PersonalDialog\citeext{Zheng2019a}和Personality Assignment\citeext{Qian2017}。这里讨论的多数研究主要是使用这三个数据集。

早期的对话生成基本是RNN+注意力机制\citeext{Bahdanau2015}的Seq2Seq模型\citeext{SutskeverGoogle2014}，如复杂的多阶段训练的\citeext{Qian2017}(数据集Personality Assignment)，简单的端到端训练的\citeext{Zheng2019a}(数据集PersonalDialog)等等，也有RNN+内存机制\citeext{Sukhbaatar2015}的Seq2Seq模型，如\citeext{Zhang2018}(数据集PERSONA-CHAT)中的几个基线模型。

随着Transformer架构的兴起，近来大部分模型都已经是Transformer架构原生的通用的或经过更改组合的专用模型，如更改GPT\citeext{Radford2018}的模型\citeext{Tselousov2018}(数据集PERSONA-CHAT)，原生GPT的\citeext{Wolf2019}(数据集PERSONA-CHAT)，加入Attention Routing的\citeext{Zheng2019}(数据集PersonalDialog)，组合GPT和BERT的\citeext{Liu2020}(数据集PERSONA-CHAT)，更改Transformer的\citeext{Roller2020}(数据集主要是PERSONA-CHAT，另外还有其他三个)等等。

此外还有相对少量的利用VAE(Variational AutoEncoder)，RL(Reinforcement Learning)和GAN(Generative Adversarial Network)等等的模型，如CVAE(Conditional VAE)+RNN+内存机制的\citeext{Song2019}(数据集PERSONA-CHAT)，VAE+GRU(RNN)+内存机制＋注意力机制的\citeext{Xu2020}(数据集PERSONA-CHAT)，RL+Transformer的\citeext{Liu2020}(数据集PERSONA-CHAT)等等，GAN相关的模型我们未作研究，在此不做介绍。

\section[Model]{模型} 

我们选用Transformer架构的Attention Routing模型\citeext{Zheng2019}作为研究对象，是基于其模型的简单性及高效，并且未偏离原生Transformer架构太远，因而能够对其运用大多数原生Transformer架构的改进。另外我们使用的数据集是PersonalDialog，但由于资源有限暂无法利用其完整的数百万数据，训练与评测只在随机分别取出的10万、2万会话上进行。全数据集的训练与评测留待将来研究。后文简称Attention Routing模型为AR模型，我们改进的模型称为AR+模型。

AR模型为Encoder Decoder结构，类似于Transformer架构的完整版\citeext{Vaswani2017}，区别是：

1. 输入数据方面，对话上下文双方发言以\_SEP特殊字符分隔拼接为一个字符串，文字为单字分割而不是用BPE或SentencePiece方法，双方发言的文字嵌入分别加上发言人的个性嵌入，然后输入Encoder，回复者的个性键值对拼接为另一个字符串嵌入后输入同一个Encoder。

2. 模型架构方面，Encoder和Decoder共享权重，Decoder只有一个attention模块，即attention routing模块，回复字符串嵌入后输入Decoder分别Attend到自身，Attention为$O_{prev}$，Attend到对话上下文的Encode，Attention为$O_C$和Attend到回复者个性Encode，Attention为$O_T$，然后三个Attention相加，$O_T$和$O_C$分别乘于权重，额外增加一项$O_C$：
\begin{equation}
O_{merge} = aO_T + (1 - a)O_C + O_C + O_{prev}    
\end{equation}

权重$a$由一个监督的Dynamic Weight Predictor子网络控制，此Predictor需要额外的监督学习，所以在我们的研究中暂不考虑，直接设置权重为1：
\begin{equation}
O_{merge} = O_T + O_C + O_{prev}    
\end{equation}

3. 训练方面，采用了多任务方式，在对话生成任务外，增加了语言模型任务，计算都是用CrossEntropy，Loss如下：
\begin{equation}
L(\phi, \theta) = L_D(\phi) + \lambda_1L_{LM}(\phi) + \lambda_2L_W(\theta)
\end{equation}

$L_D(\phi)$是对话生成Loss，$\lambda_1~\lambda_2$是Loss权重超参数，$L_{LM}(\phi)$是语言模型Loss，\\$L_W(\theta)$是Predictor的Loss，这里不予考虑，所以Loss是：
\begin{equation}
L(\phi) = L_D(\phi) + \lambda_1L_{LM}(\phi)
\end{equation}

更为详尽的描述请查看原论文\citeext{Zheng2019}。\\

AR+模型在保留AR模型整体结构基础上，引入了以下改进：

% a simple architecture change of gating each residual connection using a single zero-initialized parameter, and removed all norms except the pre norm.
1. ReZero\citeext{Bachlechner2020}方法，非常简单的架构修改，使用单个零初始化参数对每个残余连接进行门控，并移除pre norm之外的所有norms。加速模型收敛。AR+模型的处理与ReZero原论文有两个不同之处，a. 原论文没有保留pre norm， b. AR+在残余连接处为Attention~$O_T~O_C$作了Attention固定:
\begin{equation}
O_{output} = E_{prev} + bO_T + bO_C + d(O_{merge}) * r
\end{equation}

$O_{output}$是残余连接的输出，$E_{prev}$是上层输出，$b$是固定Attention超参数，默认为0.1，$d(*)$是dropout函数，$r$是零初始化的学习参数。

2. ALBERT\citeext{Lan2019}方法，因式分解嵌入层，减小嵌入维度，并使其独立于模型隐藏维度，两者因此能够分开修改；共享Transformer层的权重。减少计算量、模型尺寸及显存消耗。

3. Factor FF方法，因式分解Transformer层内的两个全连接层。减少计算量、模型尺寸及显存消耗。

4. MemN2N\citeext{Sukhbaatar2015}方法，回复者个性键值对字符串不存在顺序问题，其嵌入也较简单，不需要用Transformer编码，改为按词分割以独立的内存机制处理性能更好，且减少了计算量。

5. BART MLM\citeext{Lewis2019}方法，BART论文中表明Mask语言模型性能多数情况下比自回归语言模型更好，所以在多任务训练中我们采用与BART类似的Mask语言模型任务。

\section[Experiments]{实验} 
AR+模型的超参数如下：

字分割的词汇大小为9489，嵌入维度为200，使用PersonalDialog完整数据集预训练嵌入，词分割的个性词汇大小为10004，个性嵌入维度与模型隐藏维度相同为512，Transformer层数为6，Attention头为8，全连接维度为2048，dropout为0.1。
批大小为64，Epoch为3，优化器为AdamW(使用Transformers类库correct bias的\\AdamW\footnote{\url{https://github.com/huggingface/transformers/}})，通过lr finder\citeext{Smith2015}(类库\footnote{\url{https://github.com/davidtvs/pytorch-lr-finder}})发现学习率可为0.2e-2，权重衰退为0.05，clip grad为1，
语言模型$L_{LM}$的$\lambda_1$为0.5，学习率调度器为ReduceLROnPlateau，其超参数分别为：mode min, factor 0.5, min\_lr 1.5e-4, patience 60，MemN2N的超参数分别为：hops 3，layer\_share adjacent。

基线为AR模型，学习率为1.5e-4，其他超参数与AR+一致。如很多人说过的，学习率确实是所有超参数中影响最大的，AR模型如果和AR+用一样的0.2e-2的大学习率，则AR模型根本无法学习，而且可能由于我们用的训练数据子集不够大，致使训练第一个Epoch后就过拟合了。

基线与AR+模型的decoding策略均为Nucleus Sampling\citeext{Holtzman2019}，超参数是：temperature 0.7, top\_k 0, top\_p 0.9。

\subsection[Datasets]{数据集} 
PersonalDialog数据集原始数据包含部分重复数据，去重后是5,195,149个会话，训练、验证和测试数据集是分别在数据去重后随机无置回取出的10万、2万、2万会话子集。我们用验证集调整超参数，在测试集上进行评测。

发言普遍是短句，平均长度为15个字，所以发言长度限定为最长15个字，上下文限定为最大3轮，加上问话，上下文输入最长为$15*(2*3+1)=105$个字，回复长度同样限定15个字，个性包括性别、地址、兴趣，兴趣可包含多项。

\subsection[Training]{训练}
我们研究的是不同方法的优化效果，因此实验中如无说明均不包含预训练。

\subsection[Evaluation]{评测} 
我们只分析了自动度量，度量方法包括：1. BLEU\citeext{Papineni2002}，2. F1，3. PPL(Perplexity)，4. Dist.(Distinct)\citeext{Li2016}，以及训练速度、显存消耗和模型尺寸。人工评估留待将来研究。

\subsection[Result]{结果} 
AR+模型在大部分度量指标上均优于AR模型，Dist1和Dist2两个模型相差不大，可能是由于训练数据集较小且训练不足的原因，具体数据参见表：\ref{tab:result-comparisons}。

\begin{table*} [b]
\centering
\caption{实验结果，加粗的数据为改进指标}
%\setlength{\arraycolsep}{5pt}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l || c|c|c|c|c || c|c|c |}
\hline
\textbf{模型} & BLEU & F1 & PPL 
    & Dist1 & Dist2 & 参数 & 显存 & 训练时间 \\
\hline
AR  & 0.00257 & 0.00017 & 614 
    & 0.842 & 0.772 & 30M & 7920M & 30.5m \\
AR+ & \textbf{0.00510} & \textbf{0.00024} & \textbf{120} 
    & 0.830 & 0.780 & 31M & \textbf{5600M} & \textbf{21.0m} \\
\hline
\end{tabular}%
}
\label{tab:result-comparisons}
\end{table*}


\subsection[Ineffective Methods]{无效方法} 
下面是对无效方法的分析：

% an alternative lightweight fine-tuning strategy that achieves on-par performance to full fine-tuning. They consist of a small set of additional newly initialized weights at every layer of the transformer. These weights are then trained during fine-tuning, while the pre-trained parameters of the large model are kept frozen/fixed. 
1. Adapters\citeext{Houlsby2019}，替代微调整个模型的轻量级微调策略，微调时冻结全部预训练的参数，在Transformer层之间嵌入新的全连接层，只更新这些新层的权重。可能由于AR+模型内部不同于Transformer或预训练数据及时间不够充分，Adapters这种方法运用在AR+上后Loss太大无法完成微调。

2. 预训练特征\citeext{Devlin2019}，AR+模型的嵌入部分用预训练ALBERT\citeext{Lan2019}或ELECTRA\citeext{Clark2020}的特征替换，Loss改进十分有限甚至更差，我们还试验了特征+嵌入、特征替换Encoder、不同层的特征、不同层特征的合并、不同大小的预训练模型，结果也相差不大。

3. 预训练权重初始化\citeext{Ziegler2019}，用预训练ALBERT或ELECTRA的权重初始化AR+模型的权重，Loss改进也很有限有时更差，我们还试验了取消Encoder和Decoder共享让Encoder或Decoder单独初始化、取消层共享用预训练模型相应的层初始化、不同大小的预训练模型，结果也相差不大。

2和3的方法分别在\citeext{Devlin2019}和\citeext{Zhao2019}论文中验证有效，但却无法对AR+模型产生效果，我们还试验了BERT或XLNet\citeext{Yang2019}的预训练模型，效果也没多大变化，我们猜想可能是因为a. 这些都是BERT分支的预训练模型适合做各种分类、提取式知识问答和回归等任务，不太适合用来迁移到文本生成的任务上，尤其是开放领域对话生成任务，尽管\citeext{Zhao2019}在对话模型中成功运用了BERT，但他们的对话模型是比较特殊的VAE+RNN模型，b. 这些预训练模型的训练数据是普通的网页或文章内容，与微调的对话数据相差比较大，所以能产生的正面效果非常有限。

我们使用的中文预训练模型全部来自Transformers类库\citeext{Wolf2019HuggingFacesTS}。

另外在某些情况下预训练其实不一定有必要，首先我们看看预训练本来的目的：

1. 通用模型预训练之后，可用来微调不同的任务，达到重复利用的目的。

2. 标记数据集不够大的情况下，预训练可以提高性能，并缩短微调时间。

如果标记数据集足够大，达到500万以上，模型是专用的模型，这时如果有现成预训练过的通用模型，那可以尝试利用，但如果没有现成的适合的预训练模型，自己对专用模型预训练就显得不必要，首先无法实现1的目的，其次2的目的也未必能达到，毕竟有大规模的标记数据，用和预训练+微调同样甚至更短的时间从头训练，得到的性能也许相差无几。


\section[Ablation Study]{消融研究} 
我们分别去除五项优化中的每一项进行消融研究，和完整AR+模型对比后，可以看到这些优化分别在%减少计算量、
提高训练速度、减小模型尺寸、减小显存消耗和提高性能多个方面产生了较显著的效果，消融后对应指标的数据产生了相应的变化，具体数据参见图表：\ref{tab:result-ablation}。
 
\begin{table*} [b]
\centering
\caption{消融研究结果，加粗的数据为退化指标}
%\setlength{\arraycolsep}{5pt}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l || c|c|c|c|c || c|c|c |}
\hline
\textbf{模型} & BLEU & F1 & PPL 
    & Dist1 & Dist2 & 参数 & 显存 & 训练时间 \\
\hline
AR+ & 0.00510 & 0.00024 & 120 
    & 0.830 & 0.780 & 31M & 5600M & 21.0m \\
-ReZero & \textbf{0.00371} & \textbf{0.00019} & \textbf{172} 
        & 0.821 & 0.746 & 31M & 5360M & 19.5m \\
-ALBERT & \textbf{0.00473} & 0.00027 & \textbf{123} 
          & 0.850 & 0.784 & \textbf{45M} & \textbf{5890M} & 20.4m \\
-Factor\_FF & \textbf{0.00345} & 0.00024 & 110 
             & 0.875 & 0.789 & \textbf{33M} & \textbf{6690M} & \textbf{24.6m} \\
-MemN2N & \textbf{0.00260} & \textbf{0.00021} & 117 
          & 0.878 & 0.807 & 9M & \textbf{7410M} & \textbf{25.3m} \\
-BART\_MLM & \textbf{0.00499} & 0.00024 & \textbf{636} 
            & 0.831 & 0.780 & 31M & 5120M & 16.7m \\
\hline
\end{tabular}%
}
\label{tab:result-ablation}
\end{table*}
                     

\section[Conclusion]{结论} 
本论文对专用个性化对话生成模型运用近来Transformer架构的各项改进方法进行增强，实验表明，即使同是Transformer架构，只要内部结构有所改变，适用于原生架构的优化不一定都适用于新模型，另外BERT分支的预训练模型不适于迁移到对话生成Transformer模型。由于我们只使用小模型对PersonalDialog数据集子集进行训练，未来研究中可以扩大模型训练完整数据，以检测本论文提出的五项有效优化在大规模训练中是否依然有效。

% \section[Acknowledgement]{致谢}

\bibliographystyle{plain}
\bibliography{./persona.bib}
\clearpage\end{CJK*}
\end{document}
