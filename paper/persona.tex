\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{natbib}
\usepackage{hyperref}       % hyperlinks
% https://mirrors.tuna.tsinghua.edu.cn/CTAN/macros/latex/contrib/hyperref/doc/backref.pdf
\usepackage[hyperpageref]{backref}

\usepackage{amsmath}
\usepackage{IEEEtrantools}

% \usepackage[sort&compress,square,comma,numbers]{natbib}

\usepackage{CJKutf8}
\AtBeginDvi{\input{zhwinfonts}}

% all custom newcommand should begin with prefix: K
\newcommand{\KModelName}{RAR模型}
\newcommand{\KFactorNoused}{\underline{时间、对话环境、对方地点、发言者地点}}

% \DeclareRobustCommand{\citeext}[1]{\citeauthor{#1}~\cite{#1}}
\DeclareRobustCommand{\citeext}[1]{\cite[#1]{#1}}

\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
\setcounter{secnumdepth}{0}  
\linespread{1.2}

\begin{document}
\begin{CJK*}{UTF8}{gbsn}

\title{开放领域个性化对话生成专用模型的改进}
\author{胡俊波，韩强，刘华伟 \\
杭州高登影视公司\\
% \author{Jing Bo Hu, Qiang Han~\& Hua Wei Liu \\
% HuggingFace  Inc.\\
{\tt \{hjb,gvvvv,lhw\}@163.com} \\}

\maketitle
\begin{abstract}
本论文主要分析Transformer架构近几年来的一些速度和性能上的改进方法，其在专用模型训练上的运用，这里研究的专用模型是指个性感知的开放领域对话生成模型，针对的数据集是多轮短对话, 单个输入系列总长度不超过90个字，因此Transformer架构在长文本处理上架构、注意力机制等方面的诸多改进不在本论文讨论之列。
\end{abstract}

\section[Background]{背景} 
NLP(自然语言处理)领域的革命从注意力机制\citeext{Bahdanau2015}的奠基，Transformer架构\citeext{Vaswani2017}点燃了导火索，到BERT模型\citeext{Devlin2019}正式拉开革命帷幕，以简单的纯注意力机制架构取代了曾经是NLP主流的相对复杂的RNN架构系列模型，并在模型性能、扩展性以及速度上都取得了极大的提升，４年间全面改变了NLP领域，造就了NLP领域的ImageNet时刻\citeext{ruder2018nlpimagenet}。

开放领域对话生成是NLP的一项重要且十分复杂的任务，Transformer架构在这项任务上同样取得了质的飞跃，如GPT-2、GPT-3模型和Google Meena\citeext{Radford2019, Brown2020, Adiwardana2020}。后文如无指定说明，涉及模型皆为Transformer架构模型。

GPT-2、GPT-3模型都是通用模型，适用于几乎所有NLP任务，并非专用于开放领域对话生成，这类模型通常需以巨大的模型尺寸、海量预训练数据和漫长的预训练时间为代价，才能在性能上与专用中小模型相匹配（尽管有DistilGPT2\footnote{\url{https://github.com/huggingface/transformers/tree/master/examples/distillation}}，DistilBERT\citeext{Sanh2019}对通用模型的优化，但其性能也相应下降）。
而且通用模型很难生成个性一致的对话\footnote{即聊天机器人带有明确固定个性特征如姓名性别地址等等的个性化对话}，即使如微调GPT-2模型\citeext{Radford2019}的方法TransferTransfo\citeext{Wolf2019}\footnote{在输入的对话上下文中增加了个性化数据}，虽然其在个性化对话英文数据集PERSONA-CHAT\citeext{Zhang2018}上表现不错，但在个性化对话中文数据集PersonalDialog\citeext{Zheng2019a}上测试时仍然不如专用模型\citeext{Zheng2019}。

本论文即对开放领域个性化对话生成专用模型\citeext{Zheng2019}引入Transformer架构近几年的速度和性能上的一些改进，并分析在此专用模型上有效和无效的各种尝试，实验的源代码已经开源\footnote{\url{https://github.com/ghosthamlet/persona}}。后文简称开放领域个性化对话生成为个性化对话生成。

\section[Related Works]{相关研究} 
个性化对话生成的研究相对于无个性对话生成和闭合领域任务型对话生成来说比较少，因此相关的对话数据集也较少。英文的主要是PERSONA-CHAT\citeext{Zhang2018}，中文的有PersonalDialog\citeext{Zheng2019a}和Personality Assignment\citeext{Qian2017}。这里讨论的多数研究主要是使用这三个数据集。

早期的对话生成基本是RNN+注意力机制\citeext{Bahdanau2015}的Seq2Seq模型\citeext{SutskeverGoogle2014}，如复杂的多阶段训练的\citeext{Qian2017}(数据集Personality Assignment)，简单的端到端训练的\citeext{Zheng2019a}(数据集PersonalDialog)等等，也有RNN+内存机制\citeext{Sukhbaatar2015}的Seq2Seq模型，如\citeext{Zhang2018}(数据集PERSONA-CHAT)中的几个基线模型。

随着Transformer架构的兴起，近来大部分模型都已经是Transformer架构原生的通用的或经过更改组合的专用模型，如更改GPT\citeext{Radford2018}的模型\citeext{Tselousov2018}(数据集PERSONA-CHAT)，原生GPT的\citeext{Wolf2019}(数据集PERSONA-CHAT)，加入Attention Routing的\citeext{Zheng2019}(数据集PersonalDialog)，组合GPT和BERT的\citeext{Liu2020}(数据集PERSONA-CHAT)等等。

此外还有相对少量的利用VAE(Variational AutoEncoder)，RL(Reinforcement Learning)和GAN(Generative Adversarial Network)等等的模型，如CVAE(Conditional VAE)+RNN+内存机制的\citeext{Song2019}(数据集PERSONA-CHAT)，VAE+GRU(RNN)+内存机制＋注意力机制的\citeext{Xu2020}(数据集PERSONA-CHAT)，RL+Transformer的\citeext{Liu2020}(数据集PERSONA-CHAT)等等，GAN相关的模型我们未作研究，在此不做介绍。

我们选用Transformer架构的Attention Routing模型\citeext{Zheng2019}作为研究对象，是基于其模型的简单性及高效，并且未偏离原生Transformer架构太远，因而能够对其运用大多数原生Transformer架构的改进。另外我们使用的数据集是PersonalDialog，但由于资源有限暂无法利用其完整的数百万数据，训练与评测只在随机分别取出的10万会话上进行。全数据集的训练与评测留待将来研究。后文简称Attention Routing模型为AR模型，我们改进的模型称为AR+模型。

\section[Model]{模型} 
AR模型为Encoder Decoder结构，类似于Transformer架构的完整版\citeext{Vaswani2017}，区别是：

1. 输入数据方面，对话上下文双方发言以\_SEP特殊字符分隔拼接为一个字符串，文字为单字分割而不是用BPE或SentencePiece方法，双方发言的文字嵌入分别加上发言人的个性嵌入，然后输入Encoder，回复者的个性键值对拼接为另一个字符串输入Encoder。

2. 模型架构方面，Encoder和Decoder共享权重，Decoder只有一个attention模块，即attention routing模型，回复字符串输入Decoder分别Attend到自身Attention为$O_{prev}$、Attend到对话上下文的Encode Attention为$O_C$和Attend到回复者个性Encode Attention为$O_T$，然后三个Attention相加，$O_T$和$O_C$分别乘于权重，额外增加一项$O_C$：
\begin{equation}
O_{merge} = aO_T + (1 - a)O_C + O_C + O_{prev}    
\end{equation}

权重$a$由一个监督的Dynamic Weight Predictor子网络控制，此Predictor需要额外的监督学习，所以在我们的研究中暂不考虑，直接设置权重为1：
\begin{equation}
O_{merge} = O_T + O_C + O_{prev}    
\end{equation}

3. 训练方面，采用了多任务方式，在对话生成任务外，增加了语言模型任务，Loss如下：
\begin{equation}
L(\phi, \theta) = L_D(\phi) + \lambda_1L_{LM}(\phi) + \lambda_2L_W(\theta)
\end{equation}

$L_D(\phi)$是对话生成Loss，$L_{LM}(\phi)$是语言模型Loss，$L_W(\theta)$是Predictor的Loss，这里不考虑，所以Loss是：
\begin{equation}
L(\phi) = L_D(\phi) + L_{LM}(\phi)
\end{equation}

更为详尽的描述请查看原论文\citeext{Zheng2019}。\\

AR+模型在保留AR模型整体结构基础上，引入了以下改进：

% a simple architecture change of gating each residual connection using a single zero-initialized parameter, and removed all norms except the pre norm.
1. ReZero\citeext{Bachlechner2020}，非常简单的架构修改，使用单个零初始化参数对每个残余连接进行门控，并移除pre norm之外的所有norms。加速模型收敛。

2. ALBERT\citeext{Lan2019}, 因子分解嵌入层，减小嵌入维度，并使其独立于模型隐藏维度，两者因此能够分开修改；共享Transformer层的权重。减少计算量、模型尺寸及显存消耗。

3. Factor FF, 因子分解Transformer层内的两个全连接层。减少计算量、模型尺寸及显存消耗。

4. MemN2N\citeext{Sukhbaatar2015}，回复者个性键值对字符串不存在顺序问题，其嵌入也较简单，没必要用Transformer编码，改为按词分割以独立的内存机制处理性能更好，且减少了计算量。

5. BART MLM\citeext{Lewis2019}，BART论文中表明Mask语言模型性能多数情况下比回归语言模型更好，所以在多任务训练中我们采用与BART类似的Mask语言模型任务。


\section[Experiments]{实验} 
\subsection[Datasets]{数据集} 

\section[Ablation Study]{消融研究} 

\section[Conclusion]{结论} 
揭示预训练在大规模标记数据和专用模型上的适用问题。

\section[Acknowledgement]{致谢}

\bibliographystyle{plain}
\bibliography{./persona.bib}
\clearpage\end{CJK*}
\end{document}
