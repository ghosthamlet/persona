
# use cpu for easy debug
device: cuda
seed: 42
n_epochs: 2
# for LM
#n_epochs_early_stage: 1
n_epochs_early_stage: 0
batch_size: 64
# for LM
#batch_size: 1280
limit_example_length: 1000000
#limit_example_length: 100000
# lm context size is equal max_seq_length
max_seq_length: 15
# must be even, and >= 2
max_context_size: 6
shuffle_data: True
max_vocab_size: 42000
pretrain_emb: True

emb_freeze: False
emb_dim: 200
dropout: 0.1
num_layers: 6
n_head: 8
d_model: 512
d_ff: 2048
attn_alpha: 1
adapter_d_ff: 2048

#lr: 1.5e-4
lr: 0.2e-2
weight_decay: 0.05
clip_grad: 1
use_scheduler: True
warmup_steps: 0
gradient_accumulation: 1
adapter_finetune: False

model_path: models/
# lm clip_grad
pretrained_path: 
#pretrained_path: models/model_lm_epoch4_all_factor_ff_epoch1/model.pt
#pretrained_path: models/model_lm_epoch4_all/model.pt
# 31476781 based on 2m
#pretrained_path: models/model_lm_epoch5_2m_all/model.pt
#pretrained_path: models/model_lm_epoch9_2m/model.pt
# data_path: tmp/
data_path: datas/
cache_path: caches/
corpus_fname:
vec_fname: models/vec-char-200.txt
vocab_fname: models/vocab-char-200.txt


