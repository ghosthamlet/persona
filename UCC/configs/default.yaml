
# use cpu for easy debug
device: cuda
seed: 42
n_epochs: 10
n_epochs_early_stage: 10
# n_epochs_early_stage: 0
# batch_size: 64
# for LM
batch_size: 1024
limit_example_length: 
# most weibo are short 
# lm context size is equal max_seq_length
max_seq_length: 15
# must be even, and >= 2
max_context_size: 6
# max_context_size: 2
shuffle_data: True
# in word emb mode, english letter will removed in max_vocab_size by utils.vocab_zh_trim_rule
max_vocab_size: 42000
# pretrain char LM 440674938 n_token with 14023 n_vocab, data are [AssignPersona weibo, tieba, douban]
pretrain_emb: True

emb_freeze: True
enc_dropout: 0.1
dec_dropout: 0.1
num_layers: 6
n_head: 8
d_model: 512
d_ff: 2048
attn_alpha: 1

lr: 1.5e-4
weight_decay: 0.05
clip_grad: 1

model_path: models/
pretrained_path: models/model_lm_epoch9/model.pt
# data_path: tmp/
data_path: datas/
cache_path: caches/
corpus_fname:
vec_fname: models/vec-char.txt
vocab_fname: models/vocab-char.txt


