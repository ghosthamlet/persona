
# use cpu for easy debug
device: cuda
seed: 42
n_epochs: 10
n_epochs_early_stage: 10
clip_grad: 1
# batch_size: 128
# for LM
batch_size: 64
limit_example_length: 10000
# most weibo are short 
# lm context size is equal max_seq_length * max_context_size
max_seq_length: 15
max_context_size: 6
shuffle_data: True
# pretrain_emb must include special_tokens? see Transformers library
# english letter will removed in max_vocab_size by utils.vocab_zh_trim_rule
max_vocab_size: 42000
# pretrain char LM 440674938 n_token with 14023 n_vocab, data are [AssignPersona weibo, tieba, douban]
pretrain_emb: True

emb_freeze: True
enc_dropout: 0.1
dec_dropout: 0.1
num_layers: 6
n_head: 8
d_model: 512
d_ff: 2048
attn_alpha: 1

lr: 1.5e-4
weight_decay: 0.05

model_path: models/
pretrained_path:
# data_path: tmp/
data_path: datas/
cache_path: caches/
corpus_fname:
vec_fname: models/vec-512a.txt
vocab_fname: models/vocab-512a.txt


