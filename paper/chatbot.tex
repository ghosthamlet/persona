\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{natbib}

\usepackage{amsmath}
\usepackage{IEEEtrantools}

%% install LaTex: http://www.tug.org/texlive/quickinstall.html
%% For chinese: https://tex.stackexchange.com/questions/17611/how-does-one-type-chinese-in-latex
% \usepackage{CJKutf8}
% \AtBeginDvi{\input{zhwinfonts}}
% \begin{document}
% \begin{CJK*}{UTF8}{gbsn}
%   chinese texts
% \clearpage\end{CJK*}
% \end{document}

\usepackage{CJKutf8}
\AtBeginDvi{\input{zhwinfonts}}

% all custom newcommand should begin with prefix: K
\newcommand{\KModelName}{UCC模型}
\newcommand{\KFactorNoused}{\underline{时间、对话环境、对方地点、发言者地点}}

\frenchspacing  %Required
% \setlength{\pdfpagewidth}{8.5in}  %Required
% \setlength{\pdfpageheight}{11in}  %Required
\setcounter{secnumdepth}{0}  

\begin{document}
\begin{CJK*}{UTF8}{gbsn}

\title{更像人而不是更智能：对话系统的统一}
\author{Jing Bo Hu, Qiang Han \& Hua Wei Liu \\
HuggingFace  Inc.\\
{\tt \{hu,gvvvv,liu\}@163.com} \\}

\maketitle
\begin{abstract}
  \ldots
\end{abstract}

\section{简介}
所有对话系统除了闭合领域外包括闲聊、开放领域都需要一个统一的理论框架，以厘清目前人机对话研究领域各自为阵的混乱局面，建立简洁清晰可执行的一致的研究基础和比较完整的人机对话模型。

目前研究方向主要的两个问题：一、智能体追求更多技能更多知识，更加趋向全能的机器智能，而不是能力有限且千差万别的人类智能；二、智能体没有身份，没有个性与情绪，或者这些人类必不可少的特征被当作相互无关的独立存在而被分开研究与训练，要么就是只关心学习其浮浅的外在表示而完全忽略其内在的特质与起因，如移情对话模型（没有产生移情的内在特质及起因，移情模型只有皮肉而没有灵魂，并只能依靠巨大的数据集和模型才可能在各种情境下表现的足够好，显得无比笨拙迟钝而且不可信——就像你看到了魔术的幕后而对它嗤之以鼻那样，这个问题同样存在于其他人类特征细分模型上），同样导致机器化。这与人机对话最初愿望背道而驰，我们大多数时候需要对话的对象本质上应该是更像人类，具备人类各要素的智能体——即使它没有任何专业知识和技能只像个6岁小孩或30岁的文盲，而不是一部能模仿人类说话的万能机器或大百科全书。
人工智能的基础设施与计算能力类似于先天智能业已具备，随着知识和经验的增长，后天的智能也自然随之增长。

为此，针对以上两个问题，我们提出一个具体的解决方案，首先介绍理论框架，其次是模型与训练方法，最后是与其他基线模型的对比实验及结果的定量与定性分析。

\section{相关论文}

\section{背景}

\section{理论框架}

\KModelName在生成回复时模仿人类对话中可能受到的多种影响因素，这些因素都将作为条件控制输入模型，以牺牲对话质量为代价时可缺省部分或全部。

% 通过上下文可以自动推断出主题，但作为显式特征易于控制
因素包括：上下文、多主题、\KFactorNoused、双方熟悉程度、对方特征、发言者特征、对方情绪、发言者情绪、发言者记忆，

% 与对方关系 特征太复杂，暂缺
特征包括：姓名、性别、年龄、职业、学历、爱好、个性、出生地址、所在地址、(如何利用自监督实现预测时自由增加新的特征，如父母兄弟名字等等)。

除了情绪与记忆外，所有因素都是相对输入数据静态不变的，输入数据变化，它们才产生改变，

发言者情绪在首次数据输入后，将随着聊天过程由模型动态修改，

发言者情绪影响因素包括：上下文、多主题、对话环境、发言者健康状况、双方熟悉程度、对方特征、对方情绪、发言者特征(不同特征有不同权重，如个性权重最大)、上次发言者情绪、发言者记忆，

发言者记忆是发言者所有对话记录，包括与不同对方不同时间的对话记录，按主次分层为：短期工作记忆、长期潜意识记忆，

考虑到因素：\KFactorNoused，这些在对话中对回复影响力度不如其他因素，且为数据集标记这些属性很不容易，目前缺乏类似开放数据集，所以本论文暂不引入这些，留待将来研究。

以上部分或全部因素在巨大的深度学习模型中也许能自动形成于潜在空间，即使如此，也会需要极大量对话数据及大型模型加上漫长的训练时间，才有可能得到期望的效果，而且这些潜在空间的特征都是隐式的，难于解释和直接调校控制。然而通过我们的\KModelName，所有对话受制因素都可显式指定，生成的发言易于解释及控制，并可在需要时替换成不同模态的信息输入，因而能使对话生成系统更具实用性并适用于更广阔的应用范围。

\section{模型}
模型涉及很多不同的特征输入，架构设计应尽量减少标记数据的使用，因而需要充分利用word2vec、上下文表示嵌入、预训练模型、元学习、自监督、文本数据增强与合成等等技术。

模型包括多个模块：发言者特征模块、发言者情绪模块、发言者记忆模块、主题模块、编码模块、对话生成模块，所有模块需要联合训练，但每个模块内部模型可较容易的切换成不同模型，以利于分别研究和增强。


\section{训练}

\section{实验}

\section{结果}

\section{总结}

总之，人类的各种特征是相互关联且互相影响的，我们不可能要求机器像人的同时而缺乏人类必要的特征（如果这些特征是分别研究最后组合部署的，它们内部权重参数不具备本质上的相互关联，不算是一个实际的整体），我们需要把这些特征当成一个整体来研究。

性格特征：
Introversion extroversion 
内向　外向

% USE ~/work/AI/etracker/ for experiment track
实验步骤：
数据预处理
  性格特征：
    word2vec聚类, 句子均值或加值聚类，doc2vec聚类

  身份特征问话分类数据：
    也许在问话分类同时对答案进行POS或语法树解析分类，然后替换相应的语法元素

  Speed:
      most weibo are short, use mean seq len 
      new optim: lamb
      rezero
      albert 

      % prefetch_generator: https://github.com/IgorSusmelj/pytorch-styleguide/issues/5
      GPT2-chitchat
      gradient acc:
         loss.backward() 
         if n_iter % batch_size == batch_size-1:
            # here we perform out optimization step using a virtual batch size
            optim.step()
            optim.zero_grad()
            print('Total loss: ', total_loss)
            total_loss = 0.0
      finetune: adapter layer
      factor ff
      factor emb
      Training FlauBERT: pre-norm before attention
      lr_finder
      ?fastai: OneCycle, progressive unfreezing, test time argumentation, ...
      ?No pretrain, pretrain is for small labeled datasets and for reuse to finetune different targets, 
        if we have large labeled datasets and just one target,
        maybe just train labeled data(joint with pretrain objective) longer is fine
      ?gradient checkpoint

      所有模型内部即模型架构的微调，对整体性能影响不大，即不会改变loss landscape，只能影响模型运行速度
  Acc:
      模型输入与输出位置即模型内部以外位置的调整，不论大小，或模型内部较大的修改，都主要可能影响性能
      hug face
      lost in chat
      T5
      ROBERT/AlBERT/ELECTRA pretrained chinese model feature, merge or direct use as inputs
      just use pretrain feature embeddings? then output_emb can just it too
      init model with pretrain weights
        electra last layer, emb weights
        albert all layer, emb weights, this need change model to group layer share
      jointly finetune pretrain, remove model encoder
      finetune, share weights with model, i.e. combine above two
      use pretrain tokenizer to split
      init with pretrain_feature weight with normal transformer archtecture
      BERT pretrain_feature
      remove MLM task with pretrain_feature

      adopt MemN2N 
      BART pretrain objective: denoise add 0 length mask
      dialog with char emb, persona with word emb
      analysis grad, weight, activation before/after init weight with pretrain_feature 
        if finetune no effects, then the static feature must no effects

      two pretrain stage: 1. LM pretrain, 2. no persona dialog datasets pretrain

      Why not just let output attend all it's future seq, 
        let the model leakage and overfit the large datasets?
        when datasets is large, just search or copy answer in it is great

      move all methods param to single config, and pass to all class to easy customize
        but the code will be hard to reuse and not transparent

      ELECTRA pretrain
      reduce pretrain feature vocab size
      ?BPE/WordPiece or word vectors merge to char vector inputs
      GPT2 BPE, GPT pretrain

      build Persona chat framework

      Meena: Conv2D
      % maybe useless, this is like classify, but many unknown dialogue is not persona chat
      % Mask Persona Language Model: 
      %  self surpervize pretrain or add auxiliary loss by mask random persona value

      P2
      PEE
      different loss for predict similar output, 
        other like cross entropy for predict exact output,
        for chatbot or LM we should no need to predict exact output same as target. 
      archtecture
      activation function
      optimizer
  Datasets:
      pretrain char LM 440674938 n_token with 14023 n_vocab with short seq, data are [AssignPersona weibo, tieba, douban]
      persona chat data was deduplicated 

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
Option with ** prefix is prefered by following experiments
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------

OPTIM:
LM:
-----------------------------------------------------------------------------------------
**adamw
 Epoch: 01 | Time: 0m 33s
	Train Loss: 4.235 | Train PPL:  69.071
-----------------------------------------------------------------------------------------
lamb
 Epoch: 01 | Time: 0m 34s
	Train Loss: 4.259 | Train PPL:  70.709

-----------------------------------------------------------------------------------------
MAIN:
-----------------------------------------------------------------------------------------
**adamw
Epoch: 01 | Time: 10m 27s
    Train Loss: 6.361 | Train PPL: 578.656
         Val. Loss: 1.000 |  Val. PPL:   2.718

lamb
-----------------------------------------------------------------------------------------
Epoch: 01 | Time: 10m 54s
	Train Loss: 8.150 | Train PPL: 3463.910
	 Val. Loss: 1.000 |  Val. PPL:   2.718
adamw is better for most time.
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------

REZERO:
-----------------------------------------------------------------------------------------
standard, no pretrain:
Epoch: 01 | Time: 10m 33s
	Train Loss: 7.726 | Train PPL: 2266.670
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
rezero no share resweight, no pretrain:
Epoch: 01 | Time: 10m 21s
	Train Loss: 7.718 | Train PPL: 2248.748
	 Val. Loss: 1.000 |  Val. PPL:   2.718

-----------------------------------------------------------------------------------------
rezero share resweight, no pretrain:
Epoch: 01 | Time: 10m 14s
	Train Loss: 7.731 | Train PPL: 2277.823
	 Val. Loss: 1.000 |  Val. PPL:   2.718

-----------------------------------------------------------------------------------------
standard resdual + 0.1memory + 0.1persona, no pretrain (more stable then rezero version):
Epoch: 01 | Time: 10m 30s
	Train Loss: 7.695 | Train PPL: 2196.725
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
**rezero resdual + 0.1memory + 0.1persona, share resweight, no pretrain:
Epoch: 01 | Time: 10m 18s
	Train Loss: 7.658 | Train PPL: 2116.874
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
use all params except emb share:
Epoch: 01 | Time: 10m 31s
	Train Loss: 7.639 | Train PPL: 2077.168
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------

ALBERT:
-----------------------------------------------------------------------------------------
**share all layers:
Epoch: 01 | Time: 10m 10s
	Train Loss: 7.691 | Train PPL: 2189.560
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------

LARGE:
-----------------------------------------------------------------------------------------
16 header:
Epoch: 01 | Time: 10m 52s
	Train Loss: 7.688 | Train PPL: 2182.149
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
12 layer with max_context_size 2 (baseline max_context_size 6 overflow memory):
Epoch: 01 | Time: 10m 11s
	Train Loss: 7.719 | Train PPL: 2249.882
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------

PRETRAIN:
-----------------------------------------------------------------------------------------
LM base on 4 epochs all without ff weights:
Epoch: 01 | Time: 135m 35s
	Train Loss: 4.116 | Train PPL:  61.325
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------


FINETUNE, 100000 examples same as above all:
-----------------------------------------------------------------------------------------
LM with 2000000 pretrain 10 epochs:
Epoch: 01 | Time: 10m 8s
	Train Loss: 6.426 | Train PPL: 617.495
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
LM with all base on 2000000 pretrain 5 epochs:
loss too large, failed to train
----------------------------------------------------------------------------------------
LM with all pretrain 4 epochs:
loss too large, failed to train
----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
FINETUNE, all examples:
-----------------------------------------------------------------------------------------
LM with all base on 2000000 pretrain 5 epochs:
loss too large, failed to train
----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
FINETUNE, 100000 examples, with clip_grad :
----------------------------------------------------------------------------------------
**LM with all pretrain 4 epochs (LM same as above all without clip_grad):
Epoch: 01 | Time: 10m 9s
	Train Loss: 6.045 | Train PPL: 421.879
	 Val. Loss: 1.000 |  Val. PPL:   2.718
LM huge data pretrain cause large weight distribution, so finetune must use clip_grad
pretrain may be disable clip_grad for reach large probability space.
-----------------------------------------------------------------------------------------
**LM with all pretrain 4 epochs with clip_grad:
Epoch: 01 | Time: 10m 2s
	Train Loss: 6.267 | Train PPL: 526.747
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
transformers.get_linear_schedule_with_warmup:
Epoch: 01 | Time: 10m 12s
	Train Loss: 6.831 | Train PPL: 926.332
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
**transformers.AdamW:
Epoch: 01 | Time: 10m 11s
	Train Loss: 6.115 | Train PPL: 452.545
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
prefetch_generator:
Epoch: 01 | Time: 10m 14s
	Train Loss: 6.115 | Train PPL: 452.545
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
??gradient_accumulation:
Epoch: 01 | Time: 9m 56s
	Train Loss: 2.414 * 3(gradient_accumulation) | Train PPL:  11.182
	 Val. Loss: 1.000 |  Val. PPL:   2.718
Epoch: 01 | Time: 10m 15s
        Train Loss: 1.219 * 6(gradient_accumulation) | Train PPL:   3.384
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
gelu:
Epoch: 01 | Time: 10m 5s
	Train Loss: 6.178 | Train PPL: 482.134
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
adapter layer:
loss too high, maybe pretrain data and steps not enough
maybe remove rezero and share layers for pretrain
-----------------------------------------------------------------------------------------
adapter layer without pretrain (transformer block random weights):
Epoch: 01 | Time: 10m 49s
	Train Loss: 8.102 | Train PPL: 3299.585
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
**factor ff without pretrain:
Epoch: 01 | Time: 8m 1s
	Train Loss: 7.766 | Train PPL: 2358.529
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
**factor ff, repretrained 1 epoch LM base on early 4 epochs full ff pretrain:
(after small part model arch change, no need to repretrain from scratch,
the compatible model part can be loaded to repretrain)
Epoch: 01 | Time: 8m 39s
	Train Loss: 6.134 | Train PPL: 461.388
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
**factor emb 200dim without pretrain without tying and factor pre_softmax:
(proj before dropout perform a bit better)
(tying and factor pre_softmax will increase much loss)
(pretrained embeddings has few effects)
Epoch: 01 | Time: 8m 4s
	Train Loss: 7.591 | Train PPL: 1980.477
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
**pre-norm before attention no pretrain:
Epoch: 01 | Time: 8m 10s
	Train Loss: 7.422 | Train PPL: 1673.100
	 Val. Loss: 1.000 |  Val. PPL:   2.718
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
**lr_finder lr: 0.2e-2 with lr_scheduler.ReduceLROnPlateau no pretrain:
(all before are 1.5e-4 no lr_scheduler)
Epoch: 01 | Time: 8m 6s
	Train Loss: 6.820 | Train PPL: 916.251
	 Val. Loss: 1.000 |  Val. PPL:   2.718
Epoch: 01 | Time: 11m 9s
	Train Loss: 6.836 | Train PPL: 931.176
	 Val. Loss: 6.537 |  Val. PPL: 690.193
Epoch: 02 | Time: 11m 6s
	Train Loss: 6.354 | Train PPL: 574.999
	 Val. Loss: 6.413 |  Val. PPL: 609.473
| Test Loss: 6.411 | Test PPL: 608.236 |
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
(after tune datasets context to include all dialog, size larger then before 100000)
**Epoch: 01 | Time: 12m 56s
	Train Loss: 6.830 | Train PPL: 925.472
	 Val. Loss: 6.544 |  Val. PPL: 695.064
(LM auxiliary task removed context seq)
**Epoch: 01 | Time: 9m 22s
	Train Loss: 6.998 | Train PPL: 1094.685
	 Val. Loss: 6.690 |  Val. PPL: 804.099
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
(batch_size 8 pretrain_feature voidful/albert_chinese_large)
(!!! train/valid/test Loss and PPL is great, but eval metrics are bad???)
Epoch: 01 | Time: 168m 18s
	Train Loss: 5.139 | Train PPL: 170.629
	 Val. Loss: 4.409 |  Val. PPL:  82.163
Epoch: 02 | Time: 167m 53s
	Train Loss: 4.076 | Train PPL:  58.929
	 Val. Loss: 4.008 |  Val. PPL:  55.030
(batch_size 48 pretrain_feature hfl/chinese-electra-small-discriminator)
(!!! train/valid/test Loss and PPL is the best, but eval metrics are bad???)
Epoch: 01 | Time: 17m 23s
	Train Loss: 3.355 | Train PPL:  28.652
	 Val. Loss: 2.660 |  Val. PPL:  14.301
| Test Loss: 2.659 | Test PPL:  14.281 |
Epoch: 01 | Time: 17m 4s % add emb pos_encoder
	Train Loss: 3.669 | Train PPL:  39.204
	 Val. Loss: 2.862 |  Val. PPL:  17.497
Epoch: 01 | Time: 17m 30s % add proj dropout
	Train Loss: 3.786 | Train PPL:  44.096
	 Val. Loss: 2.919 |  Val. PPL:  18.516
Epoch: 01 | Time: 15m 30s % correct some special token
	Train Loss: 3.882 | Train PPL:  48.526
	 Val. Loss: 2.998 |  Val. PPL:  20.046
| Test Loss: 2.994 | Test PPL:  19.962 |
Epoch: 01 | Time: 15m 51s % add pad mask
	Train Loss: 3.703 | Train PPL:  40.576
	 Val. Loss: 2.821 |  Val. PPL:  16.789
**(encoder pretrain_feature for output_emb will attend to future token, % so removed)
Epoch: 01 | Time: 14m 44s
	Train Loss: 6.994 | Train PPL: 1089.766
	 Val. Loss: 6.684 |  Val. PPL: 799.188
| Test Loss: 6.685 | Test PPL: 800.066 |
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
**(alpha 0.1 pretrain_feature voidful/albert_chinese_small)
Epoch: 01 | Time: 14m 44s
	Train Loss: 5.132 | Train PPL: 169.395
	 Val. Loss: 4.889 |  Val. PPL: 132.859
% pretrain_feature seems like no useful ???
**(alpha 0.1 no pretrain_feature)
Epoch: 01 | Time: 9m 19s
	Train Loss: 5.132 | Train PPL: 169.287
	 Val. Loss: 4.902 |  Val. PPL: 134.573
Bleu: 0.00479822 | F1: 0.00018075 | Dist1: 0.865 | Dist2: 0.801 | PPL: 141.363
**(alpha 0.1 no pretrain_feature MLM)
Epoch: 01 | Time: 10m 34s
	Train Loss: 4.963 | Train PPL: 142.989
	 Val. Loss: 4.639 |  Val. PPL: 103.402
Bleu: 0.00390583 | F1: 0.00018943 | Dist1: 0.850 | Dist2: 0.773 | PPL:  99.721
**(alpha 0.5 as all early, no pretrain_feature with MLM % big loss improve)
Epoch: 01 | Time: 10m 51s
	Train Loss: 5.513 | Train PPL: 247.898
	 Val. Loss: 4.902 |  Val. PPL: 134.585
Epoch: 01 | Time: 10m 31s
	Train Loss: 5.579 | Train PPL: 264.831
	 Val. Loss: 4.961 |  Val. PPL: 142.791
Epoch: 01 | Time: 8m 4s
	Train Loss: 5.519 | Train PPL: 249.501
	 Val. Loss: 0.000 |  Val. PPL:   1.000
2020-08-15 23:17:13,597 - INFO - Experiment MemN2N: 
2020-08-15 23:17:13,597 - INFO - Epoch: 01 | Time: 6m 37s
2020-08-15 23:17:13,597 - INFO - 	Train Loss: 5.589 | Train PPL: 267.382
2020-08-15 23:17:13,597 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
% init encoder with weight_plus, no share encoder decoder, voidful/albert_chinese_small
% with num_layers 6 num_groups 2
*Epoch: 01 | Time: 10m 48s
	Train Loss: 5.433 | Train PPL: 228.863
	 Val. Loss: 0.000 |  Val. PPL:   1.000
| Test Loss: 5.064 | Test PPL: 158.224 |
Bleu: 0.00330082 | F1: 0.00019547 | Dist1: 0.867 | Dist2: 0.791 | PPL: 106.405
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
(alpha 0.1 pretrain_feature voidful/albert_chinese_small MLM)
Epoch: 01 | Time: 16m 43s
	Train Loss: 4.972 | Train PPL: 144.344
	 Val. Loss: 4.639 |  Val. PPL: 103.426
(alpha 0.5 pretrain_feature emb for input and output_emb voidful/albert_chinese_small MLM)
Epoch: 01 | Time: 17m 46s
	Train Loss: 5.493 | Train PPL: 243.104
	 Val. Loss: 4.942 |  Val. PPL: 140.047
**(alpha 0.5 init with pretrain_feature emb hfl/chinese-electra-small-discriminator MLM)
% self.pretrain_feature_model.requires_grad_(False)
Epoch: 01 | Time: 11m 0s
	Train Loss: 5.436 | Train PPL: 229.444
	 Val. Loss: 4.894 |  Val. PPL: 133.537

(alpha 0.5 init with pretrain_feature emb and pretrain_feature last layer att weight hfl/chinese-electra-small-discriminator MLM, % smaller model, so is fast, the loss is near same??)
% self.pretrain_feature_model.requires_grad_(False)
% init with albert the same, all these init has few effects
%% Why all the init with pretrain_feature weight has no good effects?!
%% include encoder init, decoder init, encoder decoder share init
%% init by last layer, last several layers 
%% emb init, emb init weight with other layer init, layer init without emb init
%% init with emb + pretrain_feature(last layer hid, sum layer, mean layer)
%% init with normal transformer archtecture, this get worse
%% no init, but with pretrain_feature emb/layer, or emb + pretrain_feature emb/layer
%% use pretrain tokenizer to split not simple split by space also has few effects
%% maybe albert and electra and all bert-like encoder model pretrain just not useful for dialogue generate,
%% we have to use gpt pretrain or pretrain the custom model self
%% 难道是强扭的瓜不甜？
%% BUT this paper: BERT for Open-Domain Conversation Modeling, says BERT works:
%% "we demonstrate that simply using fixed pre-trained BERT as part of the model without further finetuning is powerful enough for generating better responses in terms of fluency, grammar, and semantic coherency. Fine-tuning can achieve the comparable results."
Epoch: 01 | Time: 7m 49s
	Train Loss: 5.682 | Train PPL: 293.491
	 Val. Loss: 4.969 |  Val. PPL: 143.933
Epoch: 01 | Time: 7m 6s
	Train Loss: 5.544 | Train PPL: 255.596
	 Val. Loss: 4.929 |  Val. PPL: 138.211
Epoch: 01 | Time: 7m 0s
	Train Loss: 5.537 | Train PPL: 253.798
	 Val. Loss: 4.924 |  Val. PPL: 137.619
Bleu: 0.00314315 | F1: 0.00007013 | Dist1: 0.865 | Dist2: 0.766 | PPL: 160.456
(% emb norm and learnable pos emb useless)
no share encoder decoder
Epoch: 01 | Time: 10m 52s
	Train Loss: 5.730 | Train PPL: 308.036
	 Val. Loss: 5.028 |  Val. PPL: 152.628
no share encoder decoder, and layer
Epoch: 01 | Time: 10m 58s
	Train Loss: 5.667 | Train PPL: 289.087
	 Val. Loss: 4.879 |  Val. PPL: 131.536

**(alpha 0.5 as all early, no pretrain_feature MLM % big loss improve)
1000000 data
Epoch: 01 | Time: 79m 43s
	Train Loss: 4.748 | Train PPL: 115.346
	 Val. Loss: 4.454 |  Val. PPL:  85.968
| Test Loss: 4.454 | Test PPL:  85.999 |
Bleu: 0.00554876 | F1: 0.00021227 | Dist1: 0.823 | Dist2: 0.756 | PPL:  88.789
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
model size like electra-small, to see if speed and loss same as with pretrain_feature
but too small model will underfit the full millions datasets, so not fit the open domain task
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
pretrain_feature CLS for MemN2N post_query
2020-08-14 23:24:44,386 - INFO - Experiment MemN2N: 
2020-08-14 23:24:44,386 - INFO - Epoch: 01 | Time: 6m 7s
2020-08-14 23:24:44,386 - INFO - 	Train Loss: 5.573 | Train PPL: 263.143
2020-08-14 23:24:44,387 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
replace [mask] to random token like electra
2020-08-14 18:46:01,139 - INFO - Epoch: 01 | Time: 7m 52s
2020-08-14 18:46:01,139 - INFO - 	Train Loss: 5.692 | Train PPL: 296.551
2020-08-14 18:46:01,139 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
**MemN2N persona memory encode with sep emb, post emb query it, result add to context_emb
2020-08-14 19:39:44,932 - INFO - Epoch: 01 | Time: 5m 58s
2020-08-14 19:39:44,932 - INFO - 	Train Loss: 5.567 | Train PPL: 261.732
2020-08-14 19:39:44,932 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
% loss is a few high, but speed is 2m faster

... add to context_enc
2020-08-14 19:47:18,209 - INFO - Epoch: 01 | Time: 6m 0s
2020-08-14 19:47:18,209 - INFO - 	Train Loss: 5.591 | Train PPL: 268.136
2020-08-14 19:47:18,209 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000

MemN2N persona memory encode with sep emb, post enc query it, add to context_emb
2020-08-14 19:57:15,905 - INFO - Experiment MemN2N: 
2020-08-14 19:57:15,905 - INFO - Epoch: 01 | Time: 8m 14s
2020-08-14 19:57:15,905 - INFO - 	Train Loss: 5.587 | Train PPL: 267.038
2020-08-14 19:57:15,905 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000

... add to context_enc
2020-08-14 20:06:16,623 - INFO - Experiment MemN2N: 
2020-08-14 20:06:16,623 - INFO - Epoch: 01 | Time: 8m 12s
2020-08-14 20:06:16,624 - INFO - 	Train Loss: 5.560 | Train PPL: 259.837
2020-08-14 20:06:16,624 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000

multi MemN2N layer
2020-08-14 20:16:58,693 - INFO - Experiment MemN2N: 
2020-08-14 20:16:58,693 - INFO - Epoch: 01 | Time: 6m 5s
2020-08-14 20:16:58,693 - INFO - 	Train Loss: 5.573 | Train PPL: 263.149
2020-08-14 20:16:58,693 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000


mean post_query, above all is sum
2020-08-14 20:31:10,924 - INFO - Experiment MemN2N: 
2020-08-14 20:31:10,924 - INFO - Epoch: 01 | Time: 6m 0s
2020-08-14 20:31:10,924 - INFO - 	Train Loss: 5.585 | Train PPL: 266.524
2020-08-14 20:31:10,924 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000

*MemN2N persona memory encode with sep small transformer, post emb query, add to context_emb
2020-08-14 20:42:45,923 - INFO - Experiment MemN2N: 
2020-08-14 20:42:45,923 - INFO - Epoch: 01 | Time: 8m 56s
2020-08-14 20:42:45,923 - INFO - 	Train Loss: 5.544 | Train PPL: 255.575
2020-08-14 20:42:45,923 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
% loss is few good, but speed is slow

... post enc query, add to context_emb
worse

... post emb query, add to context_enc
2020-08-14 21:04:12,322 - INFO - Experiment MemN2N: 
2020-08-14 21:04:12,322 - INFO - Epoch: 01 | Time: 9m 11s
2020-08-14 21:04:12,322 - INFO - 	Train Loss: 5.625 | Train PPL: 277.411
2020-08-14 21:04:12,323 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000

MemN2N persona memory, add to context_enc and out
2020-08-14 22:42:29,814 - INFO - Experiment MemN2N: 
2020-08-14 22:42:29,814 - INFO - Epoch: 01 | Time: 8m 7s
2020-08-14 22:42:29,814 - INFO - 	Train Loss: 5.593 | Train PPL: 268.526
2020-08-14 22:42:29,814 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000

**word emb MemN2N
2020-08-17 23:00:49,687 - INFO - Experiment MemN2N: 
2020-08-17 23:00:49,688 - INFO - Epoch: 01 | Time: 6m 8s
2020-08-17 23:00:49,688 - INFO - 	Train Loss: 5.541 | Train PPL: 254.968
2020-08-17 23:00:49,688 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
    ADD pos enc to input post_query, % why this is worse?
    2020-08-17 23:25:53,077 - INFO - Experiment MemN2N: 
    2020-08-17 23:25:53,077 - INFO - Epoch: 01 | Time: 6m 9s
    2020-08-17 23:25:53,078 - INFO - 	Train Loss: 5.560 | Train PPL: 259.709
    2020-08-17 23:25:53,078 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
    **mem_n2n_layer_share = 'adjacent'
    2020-08-17 23:43:49,106 - INFO - Experiment MemN2N: 
    2020-08-17 23:43:49,106 - INFO - Epoch: 01 | Time: 6m 6s
    2020-08-17 23:43:49,106 - INFO - 	Train Loss: 5.538 | Train PPL: 254.147
    2020-08-17 23:43:49,107 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
    Bleu: 0.00416078 | F1: 0.00016897 | Dist1: 0.849 | Dist2: 0.782 | PPL: 158.488

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
dialog with char emb, persona with word emb
2020-08-16 00:51:07,498 - INFO - Experiment MemN2N: 
2020-08-16 00:51:07,499 - INFO - Epoch: 01 | Time: 6m 40s
2020-08-16 00:51:07,499 - INFO - 	Train Loss: 5.613 | Train PPL: 274.060
2020-08-16 00:51:07,499 - INFO - 	 Val. Loss: 0.000 |  Val. PPL:   1.000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
No pretrain, train all 4 epochs:

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------

AR+
params: 31,247,409
MEM: 5600M
Time(include valid) 
2020-08-21 14:07:33,994 - INFO - Epoch: 03 | Time: 6m 59s
2020-08-21 14:07:33,994 - INFO - 	Train Loss: 4.718 | Train PPL: 111.920
2020-08-21 14:07:33,994 - INFO - 	 Val. Loss: 4.808 |  Val. PPL: 122.512
2020-08-21 14:08:00,146 - INFO - | Test Loss: 4.788 | Test PPL: 120.057 |
Bleu: 0.00509641 | F1: 0.00023654 | Dist1: 0.830 | Dist2: 0.780 | PPL: 119.998
% time * 3

AR:
params: 30,081,024
MEM: 7920
Time(include valid) 
lr: 0.2e-2 
% did not learn, and overfit
2020-08-21 11:36:36,732 - INFO - Epoch: 02 | Time: 10m 3s
2020-08-21 11:36:36,732 - INFO - 	Train Loss: 8.819 | Train PPL: 6762.262
2020-08-21 11:36:36,732 - INFO - 	 Val. Loss: 9.466 |  Val. PPL: 12913.355
Bleu: 0.00000000 | F1: 0.00010080 | Dist1: 0.792 | Dist2: 0.866 | PPL: 12429.043
lr: 1.5e-4
% Dist1两个模型相差不大，可能是由于训练数据集较小且训练不足的原因。
2020-08-21 13:20:41,817 - INFO - Epoch: 03 | Time: 10m 10s
2020-08-21 13:20:41,817 - INFO - 	Train Loss: 6.438 | Train PPL: 624.927
2020-08-21 13:20:41,817 - INFO - 	 Val. Loss: 6.449 |  Val. PPL: 632.035
2020-08-21 13:21:19,837 - INFO - | Test Loss: 6.420 | Test PPL: 613.996 |
Bleu: 0.00256675 | F1: 0.00017396 | Dist1: 0.842 | Dist2: 0.772 | PPL: 614.254


exclude #1 ReZero:
params: 31,247,409
MEM: 5360M
Time(include valid) 
% lr: 1.5e-4
2020-08-21 15:42:35,413 - INFO - Epoch: 03 | Time: 6m 29s
2020-08-21 15:42:35,413 - INFO - 	Train Loss: 6.101 | Train PPL: 446.465
2020-08-21 15:42:35,413 - INFO - 	 Val. Loss: 5.164 |  Val. PPL: 174.861
2020-08-21 15:43:01,311 - INFO - | Test Loss: 5.151 | Test PPL: 172.585 |
Bleu: 0.00371071 | F1: 0.00019026 | Dist1: 0.821 | Dist2: 0.746 | PPL: 172.474

exclude #2 ALBERT:
params: 44,764,854
MEM: 5730M
Time(include valid) 
% lr: 1.5e-4
% emb is bigger, 6 times more transformer layer, so performance is better 
% if model size is not a problem, don't use ALBERT method
% if used, add ReZero
2020-08-21 16:29:53,959 - INFO - Epoch: 03 | Time: 6m 51s
2020-08-21 16:29:53,959 - INFO - 	Train Loss: 4.775 | Train PPL: 118.520
2020-08-21 16:29:53,959 - INFO - 	 Val. Loss: 4.712 |  Val. PPL: 111.230
2020-08-21 16:30:19,654 - INFO - | Test Loss: 4.695 | Test PPL: 109.347 |
Bleu: 0.00451880 | F1: 0.00024912 | Dist1: 0.852 | Dist2: 0.773 | PPL: 109.418

exclude #3 Factor FF:
params: 54,274,566
MEM: 6410M
Time(include valid) 
% lr: 1.5e-4
2020-08-21 17:16:19,406 - INFO - Epoch: 03 | Time: 8m 17s
2020-08-21 17:16:19,407 - INFO - 	Train Loss: 4.928 | Train PPL: 138.124
2020-08-21 17:16:19,407 - INFO - 	 Val. Loss: 4.828 |  Val. PPL: 125.018
2020-08-21 17:16:50,733 - INFO - | Test Loss: 4.814 | Test PPL: 123.169 |
Bleu: 0.00360490 | F1: 0.00022391 | Dist1: 0.862 | Dist2: 0.794 | PPL: 122.758

exclude #4 MemN2N:
params: 28,646,406
MEM: 8200M
Time(include valid) 
% lr: 1.5e-4
% params is smaller, as person used emb and encoder, and persona emb share with context emb, encoder share with decoder
2020-08-21 18:16:20,324 - INFO - Epoch: 03 | Time: 10m 44s
2020-08-21 18:16:20,324 - INFO - 	Train Loss: 5.011 | Train PPL: 150.085
2020-08-21 18:16:20,324 - INFO - 	 Val. Loss: 4.887 |  Val. PPL: 132.494
2020-08-21 18:17:01,140 - INFO - | Test Loss: 4.875 | Test PPL: 131.010 |
Bleu: 0.00402485 | F1: 0.00021752 | Dist1: 0.830 | Dist2: 0.758 | PPL: 130.377

exclude #5 BART MLM (equal AR):
params: 28,646,406
MEM: 7800M
Time(include valid) 
% lr: 1.5e-4
2020-08-21 19:15:43,993 - INFO - Epoch: 03 | Time: 9m 38s
2020-08-21 19:15:43,993 - INFO - 	Train Loss: 6.342 | Train PPL: 568.178
2020-08-21 19:15:43,993 - INFO - 	 Val. Loss: 6.378 |  Val. PPL: 588.554
2020-08-21 19:16:19,808 - INFO - | Test Loss: 6.357 | Test PPL: 576.252 |
Bleu: 0.00406702 | F1: 0.00020639 | Dist1: 0.810 | Dist2: 0.746 | PPL: 576.252



身份特征问话分类：


http://nlpprogress.com/english/dialogue.html

https://ai.quantumstat.com/
https://parl.ai/projects/
https://github.com/atselousov/transformer\_chatbot
https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images


datasets:
Chinese Emotional Conversation Generation
http://coai.cs.tsinghua.edu.cn/hml/challenge/dataset\_description/


PCCM复杂模型的简单数据集可以通过profile标签提取出对应的具体profile值形成复杂数据，如positive name标签提取出：张三丰
然后用更简单的类似ProfileFusion模型训练

\clearpage\end{CJK*}
\end{document}
